{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f16d13-0df3-40de-bb21-4270573da665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43bee619-4b88-4dd4-931a-68574442e4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis for 1000 lisngs within 3 mi from usc\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://losangeles.craigslist.org/search/wst/apa\"\n",
    "usc_lat = 34.0224\n",
    "usc_lon = -118.2851\n",
    "search_distance = 3 \n",
    "target_listings = 1000\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'}\n",
    "\n",
    "print(\"analysis for\", target_listings, \"lisngs within 3 mi from usc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "804c10c5-17f4-4be1-adef-187dde482913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Page 1 (offset 0 ):\n",
      "316 litings\n",
      "316 listings w a listing ct. : 316\n",
      "\n",
      "Page 2 (offset 120 ):\n",
      "316 litings\n",
      "316 listings w a listing ct. : 632\n",
      "\n",
      "Page 3 (offset 240 ):\n",
      "316 litings\n",
      "316 listings w a listing ct. : 948\n",
      "\n",
      "Page 4 (offset 360 ):\n",
      "316 litings\n",
      "316 listings w a listing ct. : 1264\n",
      "total listings : 1264\n"
     ]
    }
   ],
   "source": [
    "all_listings= []\n",
    "offset = 0\n",
    "page_number = 1\n",
    "\n",
    "while len(all_listings) < target_listings:\n",
    "    params = {'search_distance': 3, 'postal': '90007', 's': offset}\n",
    "    print(\"\\nPage\", page_number, \"(offset\", offset, \"):\")\n",
    "    try:\n",
    "        response = requests.get(base_url, params = params, headers = headers)\n",
    "        if response.status_code != 200:\n",
    "            print(response.status_code)\n",
    "            break\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        listings_on_page = soup.find_all('li', class_='cl-static-search-result')\n",
    "        \n",
    "        if len(listings_on_page) == 0:\n",
    "            break\n",
    "        print(len(listings_on_page), \"litings\")\n",
    "        \n",
    "        page_count = 0\n",
    "        for listing in listings_on_page:\n",
    "            #this scrapes the name \n",
    "            title_elem = listing.find('div', class_='title')\n",
    "            if title_elem:\n",
    "                name = title_elem.get_text().strip()\n",
    "            else:\n",
    "                name = \"\"\n",
    "            \n",
    "            # gets the prices\n",
    "            price_elem = listing.find('div', class_='price')\n",
    "            if price_elem:\n",
    "                price = price_elem.get_text().strip()\n",
    "            else:\n",
    "                price = \"\"\n",
    "            \n",
    "            # sprapes locaton\n",
    "            location_elem = listing.find('div', class_ = 'location')\n",
    "            if location_elem:\n",
    "                location = location_elem.get_text().strip()\n",
    "            else:\n",
    "                location = \"\"\n",
    "            \n",
    "            #num ofbeds\n",
    "            housing_elem = listing.find('div', class_= 'meta')\n",
    "            bedrooms = \"\"\n",
    "            if name:\n",
    "                name_lower = name.lower()\n",
    "                #2br fornat \n",
    "                bed_match = re.search(r'(\\d+)\\s*br\\b', name_lower)\n",
    "                if bed_match:\n",
    "                    bedrooms = bed_match.group(1) + \"br\"\n",
    "                #2bed/2 bedroom format\n",
    "                if not bedrooms:\n",
    "                    bed_match = re.search(r'(\\d+)\\s*bed', name_lower)\n",
    "                    if bed_match:\n",
    "                        bedrooms = bed_match.group(1) + \"br\"\n",
    "                #studio\n",
    "                if not bedrooms and 'studio' in name_lower:\n",
    "                    bedrooms = \"studio\"\n",
    "                # studio/1B format\n",
    "                if not bedrooms:\n",
    "                    bed_match = re.search(r'(\\d+)b\\b', name_lower)\n",
    "                    if bed_match:\n",
    "                        bedrooms = bed_match.group(1) + \"br\"\n",
    "                        \n",
    "            #getting thhe url\n",
    "            link_elem = listing.find('a', href=True)\n",
    "            if link_elem:\n",
    "                url = link_elem['href']\n",
    "                if not url.startswith('http'):\n",
    "                    url = 'https://losangeles.craigslist.org' + url\n",
    "            else:\n",
    "                url = \"\"\n",
    "            \n",
    "            #zip\n",
    "            zip_code = \"\"\n",
    "            if location:\n",
    "                zip_match = re.search(r'\\b(90\\d{3})\\b', location)\n",
    "                if zip_match:\n",
    "                    zip_code = zip_match.group(1)\n",
    "            \n",
    "            #only want useful lsitings\n",
    "            if name or price:\n",
    "                listing_data = {'name': name, 'address': location, 'zip_code': zip_code, 'price_raw': price, 'bedrooms_raw': bedrooms, 'url': url}                \n",
    "                all_listings.append(listing_data)\n",
    "                page_count = page_count + 1\n",
    "        print(page_count, \"listings w a listing ct. :\", len(all_listings))\n",
    "        if len(all_listings) >= target_listings:\n",
    "            break\n",
    "            \n",
    "        offset = offset + 120\n",
    "        page_number = page_number + 1\n",
    "        \n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        break\n",
    "print(\"total listings :\", len(all_listings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8812925-6b39-47d0-b5c2-ab6078716cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listings w zip : 476 from 1264\n"
     ]
    }
   ],
   "source": [
    "neighborhood_zips = {'university park': '90007', 'usc': '90007', 'u.s.c': '90007', 'u s c': '90007', 'university of southern california': '90089', 'koreatown': '90006', 'korea town': '90006', 'ktown': '90006', 'k-town': '90006', 'pico union': '90006', 'pico-union': '90006', 'pico': '90006', 'downtown': '90015', 'dtla': '90015', 'down town': '90015', 'fashion district': '90015', 'adams': '90018', 'west adams': '90018', 'w adams': '90018', 'jefferson park': '90018', 'mid city': '90019', 'mid-city': '90019', 'midcity': '90019', 'arlington heights': '90019', 'south la': '90037', 'south los angeles': '90037', 'south l.a.': '90037', 'exposition park': '90037', 'expo park': '90037', 'historic south central': '90037', 'westlake': '90057', 'west lake': '90057', 'macarthur park': '90057', 'mac arthur park': '90057', 'koreatown north': '90020', 'mid wilshire': '90005', 'mid-wilshire': '90005', 'wilshire': '90005', 'hancock park': '90004', 'windsor square': '90004', 'koreatown east': '90010', 'echo park': '90026', 'echopark': '90026', 'silver lake': '90026', 'silverlake': '90026', 'chinatown': '90012', 'china town': '90012', 'downtown la': '90013', 'historic core': '90014', 'financial district': '90017', 'arts district': '90021', 'crenshaw': '90008', 'baldwin hills': '90008', 'leimert park': '90008', 'view park': '90008'}\n",
    "for listing in all_listings:\n",
    "    if not listing['zip_code'] and listing['address']:\n",
    "        address_lower = listing['address'].lower()\n",
    "        for neighborhood, zip_code in neighborhood_zips.items():\n",
    "            if neighborhood in address_lower:\n",
    "                listing['zip_code'] = zip_code\n",
    "                break\n",
    "\n",
    "with_zip = sum(1 for l in all_listings if l['zip_code'])\n",
    "print(\"Listings w zip :\", with_zip, \"from\", len(all_listings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8ee2188-138c-457a-b80e-f53c9845b3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num listinfs: 1264\n",
      "has names: 1264\n",
      "with addresses: 1264\n",
      "haszip codes: 476\n",
      "has prices: 1264\n",
      "has bedrooms: 696\n",
      "11 zipcodes:\n",
      "['90005', '90006', '90007', '90008', '90011', '90015', '90018', '90019', '90026', '90037', '90057']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if len(all_listings) > 0:\n",
    "    count_name = sum(1 for l in all_listings if l['name'])\n",
    "    count_address = sum(1 for l in all_listings if l['address'])\n",
    "    count_zip = sum(1 for l in all_listings if l['zip_code'])\n",
    "    count_price = sum(1 for l in all_listings if l['price_raw'])\n",
    "    count_beds = sum(1 for l in all_listings if l['bedrooms_raw'])\n",
    "    zip_codes = list(set(l['zip_code'] for l in all_listings if l['zip_code']))\n",
    "    \n",
    "    print(\"num listinfs:\", len(all_listings))\n",
    "    print(\"has names:\", count_name)\n",
    "    print(\"with addresses:\", count_address)\n",
    "    print(\"haszip codes:\", count_zip)\n",
    "    print(\"has prices:\", count_price)\n",
    "    print(\"has bedrooms:\", count_beds)\n",
    "    print(len(zip_codes), \"zipcodes:\")\n",
    "    print(sorted(zip_codes))\n",
    "    \n",
    "else:\n",
    "    print(\"no listinfgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd56153-3c08-45a5-961d-52dc9dceb961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw/apartments_raw.json\n",
      "num listings saved: 1264\n"
     ]
    }
   ],
   "source": [
    "output_file = '../data/raw/apartments_raw.json'\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(all_listings, file, indent =2)\n",
    "\n",
    "print(output_file)\n",
    "print(\"num listings saved:\", len(all_listings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594a31f9-0ea5-4d74-883d-b5794ba2d524",
   "metadata": {},
   "source": [
    "#sources:\n",
    "https://oxylabs.io/blog/scraping-real-estate-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc07c0db-38a4-4d55-b7cd-cd97d16c0efc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
